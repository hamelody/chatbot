import streamlit as st # ì²« ë²ˆì§¸ ë¼ì¸ ë˜ëŠ” ì£¼ì„/ë¹ˆ ì¤„ ì œì™¸ ì²« ë¼ì¸
# st.set_page_config ë³´ë‹¤ ë¨¼ì € ì‹¤í–‰ë˜ë©´ ì•ˆ ë˜ëŠ” importëŠ” ì•„ë˜ë¡œ ì´ë™

import os
import io
import fitz  # PyMuPDF
import pandas as pd
import docx
from pptx import Presentation
import faiss
import openai # openai íŒ¨í‚¤ì§€ ì§ì ‘ ì„í¬íŠ¸
import numpy as np
import json
import time
from datetime import datetime
from openai import AzureOpenAI, APIConnectionError, APITimeoutError, RateLimitError, APIStatusError # êµ¬ì²´ì ì¸ ì˜ˆì™¸ íƒ€ì… ì„í¬íŠ¸
from azure.core.exceptions import AzureError # Azure SDK ê³µí†µ ì˜ˆì™¸
from azure.storage.blob import BlobServiceClient
import tempfile
from werkzeug.security import check_password_hash, generate_password_hash
# from streamlit_cookies_manager import EncryptedCookieManager # ì•„ë˜ë¡œ ì´ë™
import traceback
import base64
import tiktoken

# Streamlit ì•±ì˜ ê°€ì¥ ì²« ë²ˆì§¸ ëª…ë ¹ìœ¼ë¡œ st.set_page_config() í˜¸ì¶œ
st.set_page_config(
    page_title="ìœ ì•¤ìƒëª…ê³¼í•™ ì—…ë¬´ ê°€ì´ë“œ ë´‡",
    layout="centered",
    initial_sidebar_state="auto"
)

# --- st.set_page_config() í˜¸ì¶œ ì´í›„ì— ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ---
from streamlit_cookies_manager import EncryptedCookieManager
print("Imported streamlit_cookies_manager after set_page_config.")

# --- Tiktoken ì¸ì½”ë” ë¡œë“œ ---
# (ì´ì „ê³¼ ë™ì¼)
try:
    tokenizer = tiktoken.get_encoding("o200k_base")
    print("Tiktoken 'o200k_base' encoder loaded successfully.")
except Exception as e:
    st.error(f"Tiktoken ì¸ì½”ë” ë¡œë“œ ì‹¤íŒ¨: {e}. í† í° ê¸°ë°˜ ê¸¸ì´ ì œí•œì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print(f"ERROR: Failed to load tiktoken encoder: {e}")
    tokenizer = None

# --- Base64 ì¸ì½”ë”© í•¨ìˆ˜ ì •ì˜ ---
# (ì´ì „ê³¼ ë™ì¼)
def get_base64_of_bin_file(bin_file_path):
    try:
        with open(bin_file_path, 'rb') as f:
            data = f.read()
        return base64.b64encode(data).decode()
    except FileNotFoundError:
        print(f"ERROR: ë¡œê³  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {bin_file_path}")
        return None
    except Exception as e:
        print(f"ERROR: ë¡œê³  íŒŒì¼ '{bin_file_path}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# --- ì „ì—­ ë³€ìˆ˜ ë° ê²½ë¡œ ì„¤ì • ---
# (ì´ì „ê³¼ ë™ì¼)
RULES_PATH_REPO = ".streamlit/prompt_rules.txt"
COMPANY_LOGO_PATH_REPO = "company_logo.png"
INDEX_BLOB_NAME = "vector_db/vector.index"
METADATA_BLOB_NAME = "vector_db/metadata.json"
USERS_BLOB_NAME = "app_data/users.json"
UPLOAD_LOG_BLOB_NAME = "app_logs/upload_log.json"
USAGE_LOG_BLOB_NAME = "app_logs/usage_log.json"
AZURE_OPENAI_TIMEOUT = 60.0
MODEL_MAX_INPUT_TOKENS = 128000
MODEL_MAX_OUTPUT_TOKENS = 16384
BUFFER_TOKENS = 500
TARGET_INPUT_TOKENS_FOR_PROMPT = MODEL_MAX_INPUT_TOKENS - MODEL_MAX_OUTPUT_TOKENS - BUFFER_TOKENS

# --- CSS ìŠ¤íƒ€ì¼ ---
# (ì´ì „ê³¼ ë™ì¼)
st.markdown("""
<style>
    /* (ê¸°ì¡´ CSS ìŠ¤íƒ€ì¼ ë‚´ìš© ìœ ì§€) */
    .stApp > header ~ div [data-testid="stHorizontalBlock"] > div:nth-child(2) div[data-testid="stButton"] > button {
        background-color: #FFFFFF; color: #333F48; border: 1px solid #BCC0C4;
        border-radius: 8px; padding: 8px 12px; font-weight: 500;
        width: auto; min-width: 100px; white-space: nowrap; display: inline-block;
        transition: background-color 0.3s ease, border-color 0.3s ease;
    }
    .stApp > header ~ div [data-testid="stHorizontalBlock"] > div:nth-child(2) div[data-testid="stButton"] > button:hover {
        background-color: #F0F2F5; border-color: #A0A4A8;
    }
    .stApp > header ~ div [data-testid="stHorizontalBlock"] > div:nth-child(2) div[data-testid="stButton"] > button:active {
        background-color: #E0E2E5;
    }
    .chat-bubble-container { display: flex; flex-direction: column; margin-bottom: 15px; }
    .user-align { align-items: flex-end; }
    .assistant-align { align-items: flex-start; }
    .bubble { padding: 10px 15px; border-radius: 18px; max-width: 75%; word-wrap: break-word; display: inline-block; color: black; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.08); position: relative; }
    .user-bubble { background-color: #90EE90; color: black; border-bottom-right-radius: 5px; }
    .assistant-bubble { background-color: #E9E9EB; border-bottom-left-radius: 5px; }
    .timestamp { font-size: 0.7rem; color: #8E8E93; padding: 2px 5px 0px 5px; }

    /* ë©”ì¸ ì•± ì œëª© ë° ë¶€ì œëª© (ë¡œê·¸ì¸ í›„) */
    .main-app-title-container { text-align: center; margin-bottom: 24px; }
    .main-app-title { font-size: 2.1rem; font-weight: bold; display: block; }
    .main-app-subtitle { font-size: 0.9rem; color: gray; display: block; margin-top: 4px;}

    /* ë¡œê³  ë° ë²„ì „ */
    .logo-container { display: flex; align-items: center; }
    .logo-image { margin-right: 10px; }
    .version-text { font-size: 0.9rem; color: gray; }

    /* ë¡œê·¸ì¸ í™”ë©´ ì „ìš© ì œëª© ìŠ¤íƒ€ì¼ */
    .login-page-header-container { text-align: center; margin-top: 20px; margin-bottom: 10px;}
    .login-page-main-title { font-size: 1.8rem; font-weight: bold; display: block; color: #333F48; }
    .login-page-sub-title { font-size: 0.85rem; color: gray; display: block; margin-top: 2px; margin-bottom: 20px;}
    .login-form-title { /* "ë¡œê·¸ì¸ ë˜ëŠ” íšŒì›ê°€ì…" ì œëª© */
        font-size: 1.6rem;
        font-weight: bold;
        text-align: center;
        margin-top: 10px;
        margin-bottom: 25px;
    }

    /* ëª¨ë°”ì¼ í™”ë©´ ëŒ€ì‘ */
    @media (max-width: 768px) {
        .main-app-title {
            font-size: 1.8rem; /* ëª¨ë°”ì¼ì—ì„œ ë©”ì¸ ì•± ì œëª© */
        }
        .main-app-subtitle {
            font-size: 0.8rem; /* ëª¨ë°”ì¼ì—ì„œ ë©”ì¸ ì•± ë¶€ì œëª© */
        }
        .login-page-main-title {
            font-size: 1.5rem; /* ëª¨ë°”ì¼ì—ì„œ ë¡œê·¸ì¸ í˜ì´ì§€ì˜ í”„ë¡œê·¸ë¨ ì œëª© */
        }
        .login-page-sub-title {
            font-size: 0.8rem; /* ëª¨ë°”ì¼ì—ì„œ ë¡œê·¸ì¸ í˜ì´ì§€ì˜ í”„ë¡œê·¸ë¨ ë¶€ì œëª© */
        }
        .login-form-title { /* "ë¡œê·¸ì¸ ë˜ëŠ” íšŒì›ê°€ì…" ì œëª© ëª¨ë°”ì¼ í¬ê¸° */
            font-size: 1.3rem;
            margin-bottom: 20px;
        }
    }
</style>
""", unsafe_allow_html=True)


# --- Azure í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
# (ì´ì „ê³¼ ë™ì¼)
@st.cache_resource
def get_azure_openai_client_cached():
    print("Attempting to initialize Azure OpenAI client...")
    try:
        client = AzureOpenAI(
            api_key=st.secrets["AZURE_OPENAI_KEY"],
            azure_endpoint=st.secrets["AZURE_OPENAI_ENDPOINT"],
            api_version=st.secrets["AZURE_OPENAI_VERSION"],
            timeout=AZURE_OPENAI_TIMEOUT
        )
        print("Azure OpenAI client initialized successfully.")
        return client
    except KeyError as e:
        st.error(f"Azure OpenAI ì„¤ì • ì˜¤ë¥˜: secrets.toml íŒŒì¼ì— '{e.args[0]}' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì•±ì´ ì •ìƒ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print(f"ERROR: Missing Azure OpenAI secret: {e.args[0]}")
        return None
    except Exception as e:
        st.error(f"Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}. ì•±ì´ ì •ìƒ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print(f"ERROR: Azure OpenAI client initialization failed: {e}\n{traceback.format_exc()}")
        return None

@st.cache_resource
def get_azure_blob_clients_cached():
    print("Attempting to initialize Azure Blob Service client...")
    try:
        conn_str = st.secrets["AZURE_BLOB_CONN"]
        blob_service_client = BlobServiceClient.from_connection_string(conn_str, connection_timeout=60, read_timeout=120)
        container_name = st.secrets["BLOB_CONTAINER"]
        container_client = blob_service_client.get_container_client(container_name)
        print(f"Azure Blob Service client and container client for '{container_name}' initialized successfully.")
        return blob_service_client, container_client
    except KeyError as e:
        st.error(f"Azure Blob Storage ì„¤ì • ì˜¤ë¥˜: secrets.toml íŒŒì¼ì— '{e.args[0]}' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ERROR: Missing Azure Blob Storage secret: {e.args[0]}")
        return None, None
    except Exception as e:
        st.error(f"Azure Blob í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}. ë°ì´í„° ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ERROR: Azure Blob client initialization failed: {e}\n{traceback.format_exc()}")
        return None, None

openai_client = get_azure_openai_client_cached()
blob_service, container_client = get_azure_blob_clients_cached()

EMBEDDING_MODEL = None
if openai_client:
    try:
        EMBEDDING_MODEL = st.secrets["AZURE_OPENAI_EMBEDDING_DEPLOYMENT"]
        print(f"Embedding model set to: {EMBEDDING_MODEL}")
    except KeyError:
        st.error("secrets.toml íŒŒì¼ì— 'AZURE_OPENAI_EMBEDDING_DEPLOYMENT' ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. ì„ë² ë”© ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ERROR: Missing AZURE_OPENAI_EMBEDDING_DEPLOYMENT secret.")
        openai_client = None
    except Exception as e:
        st.error(f"ì„ë² ë”© ëª¨ë¸ ì„¤ì • ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"ERROR: Loading AZURE_OPENAI_EMBEDDING_DEPLOYMENT secret: {e}")
        openai_client = None


# --- ë°ì´í„° ë¡œë“œ/ì €ì¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (Blob ì—°ë™) ---
# (load_data_from_blob, save_data_to_blob, save_binary_data_to_blob í•¨ìˆ˜ëŠ” ì´ì „ê³¼ ë™ì¼)
def load_data_from_blob(blob_name, _container_client, data_description="ë°ì´í„°", default_value=None):
    if not _container_client:
        print(f"ERROR: Blob Container client is None for load_data_from_blob ('{data_description}'). Returning default.")
        return default_value if default_value is not None else {}

    print(f"Attempting to load '{data_description}' from Blob: '{blob_name}'")
    try:
        blob_client_instance = _container_client.get_blob_client(blob_name)
        if blob_client_instance.exists():
            with tempfile.TemporaryDirectory() as tmpdir:
                local_temp_path = os.path.join(tmpdir, os.path.basename(blob_name))
                print(f"Downloading '{blob_name}' to '{local_temp_path}'...")
                with open(local_temp_path, "wb") as download_file:
                    download_stream = blob_client_instance.download_blob(timeout=60)
                    download_file.write(download_stream.readall())
                if os.path.getsize(local_temp_path) > 0:
                    with open(local_temp_path, "r", encoding="utf-8") as f:
                        loaded_data = json.load(f)
                    print(f"'{data_description}' loaded successfully from Blob: '{blob_name}'")
                    return loaded_data
                else:
                    print(f"WARNING: '{data_description}' file '{blob_name}' exists in Blob but is empty. Returning default.")
                    return default_value if default_value is not None else {}
        else:
            print(f"WARNING: '{data_description}' file '{blob_name}' not found in Blob Storage. Returning default.")
            return default_value if default_value is not None else {}
    except json.JSONDecodeError:
        print(f"ERROR: Failed to decode JSON for '{data_description}' from Blob '{blob_name}'. Returning default.")
        st.warning(f"'{data_description}' íŒŒì¼({blob_name})ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        return default_value if default_value is not None else {}
    except AzureError as ae:
        print(f"AZURE ERROR loading '{data_description}' from Blob '{blob_name}': {ae}\n{traceback.format_exc()}")
        st.warning(f"'{data_description}' ë¡œë“œ ì¤‘ Azure ì„œë¹„ìŠ¤ ì˜¤ë¥˜ ë°œìƒ: {ae}. ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        return default_value if default_value is not None else {}
    except Exception as e:
        print(f"GENERAL ERROR loading '{data_description}' from Blob '{blob_name}': {e}\n{traceback.format_exc()}")
        st.warning(f"'{data_description}' ë¡œë“œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}. ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        return default_value if default_value is not None else {}


def save_data_to_blob(data_to_save, blob_name, _container_client, data_description="ë°ì´í„°"):
    if not _container_client:
        st.error(f"Azure Blob í´ë¼ì´ì–¸íŠ¸ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ '{data_description}'ë¥¼ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ERROR: Blob Container client is None, cannot save '{blob_name}'.")
        return False
    print(f"Attempting to save '{data_description}' to Blob Storage: '{blob_name}'")
    try:
        if not isinstance(data_to_save, (dict, list)):
            st.error(f"'{data_description}' ì €ì¥ ì‹¤íŒ¨: ë°ì´í„°ê°€ JSONìœ¼ë¡œ ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…(dict ë˜ëŠ” list)ì´ ì•„ë‹™ë‹ˆë‹¤.")
            print(f"ERROR: Data for '{blob_name}' is not JSON serializable (type: {type(data_to_save)}).")
            return False

        with tempfile.TemporaryDirectory() as tmpdir:
            local_temp_path = os.path.join(tmpdir, os.path.basename(blob_name))
            with open(local_temp_path, "w", encoding="utf-8") as f:
                json.dump(data_to_save, f, ensure_ascii=False, indent=2)

            blob_client_instance = _container_client.get_blob_client(blob_name)
            print(f"Uploading '{local_temp_path}' to Blob '{blob_name}'...")
            with open(local_temp_path, "rb") as data_stream:
                blob_client_instance.upload_blob(data_stream, overwrite=True, timeout=60)
            print(f"Successfully saved '{data_description}' to Blob: '{blob_name}'")
        return True
    except AzureError as ae:
        st.error(f"Azure Blobì— '{data_description}' ì €ì¥ ì¤‘ Azure ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {ae}")
        print(f"AZURE ERROR saving '{data_description}' to Blob '{blob_name}': {ae}\n{traceback.format_exc()}")
        return False
    except Exception as e:
        st.error(f"Azure Blobì— '{data_description}' ì €ì¥ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        print(f"GENERAL ERROR saving '{data_description}' to Blob '{blob_name}': {e}\n{traceback.format_exc()}")
        return False

def save_binary_data_to_blob(local_file_path, blob_name, _container_client, data_description="ë°”ì´ë„ˆë¦¬ ë°ì´í„°"):
    if not _container_client:
        st.error(f"Azure Blob í´ë¼ì´ì–¸íŠ¸ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ '{data_description}' ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¥¼ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ERROR: Blob Container client is None, cannot save binary '{blob_name}'.")
        return False
    if not os.path.exists(local_file_path):
        st.error(f"'{data_description}' ì €ì¥ì„ ìœ„í•œ ë¡œì»¬ íŒŒì¼ '{local_file_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ERROR: Local file for binary data not found: '{local_file_path}'")
        return False

    print(f"Attempting to save binary '{data_description}' from '{local_file_path}' to Blob: '{blob_name}'")
    try:
        blob_client_instance = _container_client.get_blob_client(blob_name)
        with open(local_file_path, "rb") as data_stream:
            blob_client_instance.upload_blob(data_stream, overwrite=True, timeout=120)
        print(f"Successfully saved binary '{data_description}' to Blob: '{blob_name}'")
        return True
    except AzureError as ae:
        st.error(f"Azure Blobì— ë°”ì´ë„ˆë¦¬ '{data_description}' ì €ì¥ ì¤‘ Azure ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {ae}")
        print(f"AZURE ERROR saving binary '{data_description}' to Blob '{blob_name}': {ae}\n{traceback.format_exc()}")
        return False
    except Exception as e:
        st.error(f"Azure Blobì— ë°”ì´ë„ˆë¦¬ '{data_description}' ì €ì¥ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        print(f"GENERAL ERROR saving binary '{data_description}' to Blob '{blob_name}': {e}\n{traceback.format_exc()}")
        return False


# --- ì‚¬ìš©ì ì •ë³´ ë¡œë“œ ---
# (USERS ë¡œë“œ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼)
USERS = {}
if container_client:
    USERS = load_data_from_blob(USERS_BLOB_NAME, container_client, "ì‚¬ìš©ì ì •ë³´", default_value={})
    if not isinstance(USERS, dict) :
        print(f"ERROR: USERS loaded from blob is not a dict ({type(USERS)}). Re-initializing.")
        USERS = {}

    if "admin" not in USERS:
        print(f"'{USERS_BLOB_NAME}' from Blob is empty or admin is missing. Creating default admin.")
        USERS["admin"] = {
            "name": "ê´€ë¦¬ì", "department": "í’ˆì§ˆë³´ì¦íŒ€",
            "password_hash": generate_password_hash("diteam"),
            "approved": True, "role": "admin"
        }
        if not save_data_to_blob(USERS, USERS_BLOB_NAME, container_client, "ì´ˆê¸° ì‚¬ìš©ì ì •ë³´"):
             st.warning("ê¸°ë³¸ ê´€ë¦¬ì ì •ë³´ë¥¼ Blobì— ì €ì¥í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‹¤í–‰ ì‹œ ë‹¤ì‹œ ì‹œë„ë©ë‹ˆë‹¤.")
else:
    st.error("Azure Blob Storage ì—°ê²° ì‹¤íŒ¨. ì‚¬ìš©ì ì •ë³´ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•±ì´ ì •ìƒ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("CRITICAL: Cannot initialize USERS due to Blob client failure.")
    USERS = {"admin": {"name": "ê´€ë¦¬ì(ì—°ê²°ì‹¤íŒ¨)", "department": "ì‹œìŠ¤í…œ", "password_hash": generate_password_hash("fallback"), "approved": True, "role": "admin"}}


# --- ì¿ í‚¤ ë§¤ë‹ˆì € ë° ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
# (ì¿ í‚¤ ë° ì„¸ì…˜ ì´ˆê¸°í™” ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼)
cookies = None
cookie_manager_ready = False
print(f"Attempting to load COOKIE_SECRET from st.secrets: {st.secrets.get('COOKIE_SECRET')}")
try:
    cookie_secret_key = st.secrets.get("COOKIE_SECRET")
    if not cookie_secret_key:
        st.error("secrets.toml íŒŒì¼ì— 'COOKIE_SECRET'ì´(ê°€) ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¿ í‚¤ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ERROR: COOKIE_SECRET is not set or empty in st.secrets.")
    else:
        cookies = EncryptedCookieManager(
            prefix="gmp_chatbot_auth_v5_0/",
            password=cookie_secret_key
        )
        # .ready() í˜¸ì¶œ ì „ì— ì¿ í‚¤ ë§¤ë‹ˆì €ê°€ ì‹¤ì œë¡œ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
        # Streamlit ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ íë¦„ ìƒ, ì²« ì‹¤í–‰ ì‹œì—ëŠ” readyê°€ ì•„ë‹ ìˆ˜ ìˆìŒ
        # ì‹¤ì œ ì‚¬ìš© ì‹œì ì—ì„œ .ready()ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ëŠ” ê²ƒì´ ì•ˆì „í•  ìˆ˜ ìˆìŒ
        if cookies.ready():
            cookie_manager_ready = True
            print("CookieManager is ready.")
        else:
            print("CookieManager not ready on initial setup (may resolve on first interaction).")

except Exception as e:
    st.error(f"ì¿ í‚¤ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
    print(f"CRITICAL: CookieManager initialization error: {e}\n{traceback.format_exc()}")

SESSION_TIMEOUT = 1800
try:
    session_timeout_secret = st.secrets.get("SESSION_TIMEOUT")
    if session_timeout_secret: SESSION_TIMEOUT = int(session_timeout_secret)
    print(f"Session timeout set to: {SESSION_TIMEOUT} seconds.")
except (ValueError, TypeError):
    print(f"WARNING: SESSION_TIMEOUT in secrets ('{session_timeout_secret}') is not a valid integer. Using default {SESSION_TIMEOUT}s.")
except Exception as e:
     print(f"WARNING: Error reading SESSION_TIMEOUT from secrets: {e}. Using default {SESSION_TIMEOUT}s.")

if "authenticated" not in st.session_state:
    print("Initializing st.session_state: 'authenticated', 'user', and 'messages'")
    st.session_state["authenticated"] = False
    st.session_state["user"] = {}
    st.session_state["messages"] = []

    # ì¿ í‚¤ ë§¤ë‹ˆì €ê°€ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸ í›„ ì¿ í‚¤ ë¡œë“œ ì‹œë„
    if 'cookies' in locals() and cookies and cookies.ready():
        cookie_manager_ready = True # ì—¬ê¸°ì„œ ready ìƒíƒœê°€ ë  ìˆ˜ ìˆìŒ
        print("CookieManager became ready before cookie check.")
        auth_cookie_val = cookies.get("authenticated")
        print(f"Cookie 'authenticated' value on session init: {auth_cookie_val}")
        if auth_cookie_val == "true":
            login_time_str = cookies.get("login_time", "0")
            login_time = float(login_time_str if login_time_str and login_time_str.replace('.', '', 1).isdigit() else "0")
            if (time.time() - login_time) < SESSION_TIMEOUT:
                user_json_cookie = cookies.get("user", "{}")
                try:
                    user_data_from_cookie = json.loads(user_json_cookie if user_json_cookie else "{}")
                    if user_data_from_cookie and isinstance(user_data_from_cookie, dict):
                        st.session_state["user"] = user_data_from_cookie
                        st.session_state["authenticated"] = True
                        print(f"User '{user_data_from_cookie.get('name')}' authenticated from cookie.")
                    else:
                        print("User data in cookie is empty or invalid. Clearing auth state for cookie.")
                        st.session_state["authenticated"] = False
                        if cookies.ready(): cookies["authenticated"] = "false"; cookies["user"] = ""; cookies["login_time"] = ""; cookies.save()
                except json.JSONDecodeError:
                    print("ERROR: Failed to decode user JSON from cookie. Clearing auth state for cookie.")
                    st.session_state["authenticated"] = False
                    if cookies.ready(): cookies["authenticated"] = "false"; cookies["user"] = ""; cookies["login_time"] = ""; cookies.save()
            else:
                print("Session timeout detected from cookie. Clearing auth state for cookie.")
                st.session_state["authenticated"] = False
                st.session_state["messages"] = []
                if cookies.ready(): cookies["authenticated"] = "false"; cookies["user"] = ""; cookies["login_time"] = ""; cookies.save()
        else: # authenticated ì¿ í‚¤ê°€ "true"ê°€ ì•„ë‹Œ ê²½ìš°
             print("Authenticated cookie not set to 'true'.")
             st.session_state["authenticated"] = False # ëª…ì‹œì  ì´ˆê¸°í™”

    elif 'cookies' in locals() and cookies and not cookies.ready():
         print("CookieManager still not ready, cannot restore session from cookie on session init.")
         st.session_state["authenticated"] = False # ì¿ í‚¤ ì‚¬ìš© ë¶ˆê°€ ì‹œ ë¹„ì¸ì¦ ìƒíƒœ
    else: # cookies ê°ì²´ ìì²´ê°€ Noneì¸ ê²½ìš° (ì´ˆê¸°í™” ì‹¤íŒ¨ ë“±)
         print("CookieManager object is None, cannot restore session.")
         st.session_state["authenticated"] = False


if "messages" not in st.session_state:
    st.session_state["messages"] = []
    print("Redundant check: Initializing messages as it was not in session_state before login UI.")


# --- ë¡œê·¸ì¸ UI ë° ë¡œì§ ---
# (ë¡œê·¸ì¸ UI ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼)
if not st.session_state.get("authenticated", False):
    st.markdown("""
    <div class="login-page-header-container">
      <span class="login-page-main-title">ìœ ì•¤ìƒëª…ê³¼í•™ GMP/SOP ì—…ë¬´ ê°€ì´ë“œ ë´‡</span>
      <span class="login-page-sub-title">Made by DI.PART</span>
    </div>
    """, unsafe_allow_html=True)

    st.markdown('<p class="login-form-title">ğŸ” ë¡œê·¸ì¸ ë˜ëŠ” íšŒì›ê°€ì…</p>', unsafe_allow_html=True)

    # ë¡œê·¸ì¸ í¼ í‘œì‹œ ì „ì— ì¿ í‚¤ ë§¤ë‹ˆì € ì¤€ë¹„ ìƒíƒœ í™•ì¸
    if 'cookies' in locals() and cookies and not cookies.ready() and st.secrets.get("COOKIE_SECRET"):
        # .ready()ë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ì—¬ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹œë„ (í•„ìˆ˜ëŠ” ì•„ë‹˜)
        # cookies.ready()
        st.warning("ì¿ í‚¤ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ìƒˆë¡œê³ ì¹¨í•˜ê±°ë‚˜ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        print("Login UI: CookieManager not ready yet.")


    with st.form("auth_form_final_v4_mobile_ui_fix", clear_on_submit=False):
        mode = st.radio("ì„ íƒ", ["ë¡œê·¸ì¸", "íšŒì›ê°€ì…"], key="auth_mode_final_v4_mobile_ui_fix")
        uid = st.text_input("ID", key="auth_uid_final_v4_mobile_ui_fix")
        pwd = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", key="auth_pwd_final_v4_mobile_ui_fix")
        name, dept = "", ""
        if mode == "íšŒì›ê°€ì…":
            name = st.text_input("ì´ë¦„", key="auth_name_final_v4_mobile_ui_fix")
            dept = st.text_input("ë¶€ì„œ", key="auth_dept_final_v4_mobile_ui_fix")
        submit_button = st.form_submit_button("í™•ì¸")

    if submit_button:
        if not uid or not pwd: st.error("IDì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        elif mode == "íšŒì›ê°€ì…" and (not name or not dept): st.error("ì´ë¦„ê³¼ ë¶€ì„œë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            if mode == "ë¡œê·¸ì¸":
                user_data_login = USERS.get(uid)
                if not user_data_login: st.error("ì¡´ì¬í•˜ì§€ ì•ŠëŠ” IDì…ë‹ˆë‹¤.")
                elif not user_data_login.get("approved", False): st.warning("ê°€ì… ìŠ¹ì¸ì´ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤.")
                elif check_password_hash(user_data_login["password_hash"], pwd):
                    st.session_state["authenticated"] = True
                    st.session_state["user"] = user_data_login
                    st.session_state["messages"] = []
                    print(f"Login successful for user '{uid}'. Chat messages cleared.")
                    # ì¿ í‚¤ ì €ì¥ ì‹œì ì— ready í™•ì¸
                    if 'cookies' in locals() and cookies and cookies.ready():
                        try:
                            cookies["authenticated"] = "true"; cookies["user"] = json.dumps(user_data_login)
                            cookies["login_time"] = str(time.time()); cookies.save()
                            print(f"Cookies saved for user '{uid}'.")
                        except Exception as e_cookie_save:
                            st.warning(f"ë¡œê·¸ì¸ ì¿ í‚¤ ì €ì¥ ì¤‘ ë¬¸ì œ ë°œìƒ: {e_cookie_save}")
                            print(f"ERROR: Failed to save login cookies: {e_cookie_save}")
                    else:
                        st.warning("ì¿ í‚¤ ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ë¡œê·¸ì¸ ìƒíƒœë¥¼ ë¸Œë¼ìš°ì €ì— ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        print("WARNING: CookieManager not ready during login, cannot save cookies.")
                    st.success(f"{user_data_login.get('name', uid)}ë‹˜, í™˜ì˜í•©ë‹ˆë‹¤!"); st.rerun()
                else: st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            elif mode == "íšŒì›ê°€ì…":
                if uid in USERS: st.error("ì´ë¯¸ ì¡´ì¬í•˜ëŠ” IDì…ë‹ˆë‹¤.")
                else:
                    USERS[uid] = {"name": name, "department": dept,
                                  "password_hash": generate_password_hash(pwd),
                                  "approved": False, "role": "user"}
                    if not save_data_to_blob(USERS, USERS_BLOB_NAME, container_client, "ì‚¬ìš©ì ì •ë³´"):
                        st.error("ì‚¬ìš©ì ì •ë³´ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
                        USERS.pop(uid, None)
                    else:
                        st.success("ê°€ì… ì‹ ì²­ ì™„ë£Œ! ê´€ë¦¬ì ìŠ¹ì¸ í›„ ë¡œê·¸ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    st.stop()


# --- ì¸ì¦ í›„ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œì§ ---
current_user_info = st.session_state.get("user", {})

# --- í—¤ë” (ë¡œê³ , ë²„ì „, ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼) ---
# (í—¤ë” ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼)
top_cols_main = st.columns([0.7, 0.3])
with top_cols_main[0]:
    if os.path.exists(COMPANY_LOGO_PATH_REPO):
        logo_b64 = get_base64_of_bin_file(COMPANY_LOGO_PATH_REPO)
        if logo_b64:
            st.markdown(f"""
            <div class="logo-container">
                <img src="data:image/png;base64,{logo_b64}" class="logo-image" width="150">
                <span class="version-text">ver 0.9.2 (Tiktoken Fix)</span>
            </div>""", unsafe_allow_html=True) # ë²„ì „ ì—…ë°ì´íŠ¸
        else:
            st.markdown(f"""<div class="logo-container"><span class="version-text" style="font-weight:bold;">ìœ ì•¤ìƒëª…ê³¼í•™</span> <span class="version-text" style="margin-left:10px;">ver 0.9.2 (Tiktoken Fix)</span></div>""", unsafe_allow_html=True)
    else:
        print(f"WARNING: Company logo file not found at {COMPANY_LOGO_PATH_REPO}")
        st.markdown(f"""<div class="logo-container"><span class="version-text" style="font-weight:bold;">ìœ ì•¤ìƒëª…ê³¼í•™</span> <span class="version-text" style="margin-left:10px;">ver 0.9.2 (Tiktoken Fix)</span></div>""", unsafe_allow_html=True)

with top_cols_main[1]:
    st.markdown('<div style="text-align: right;">', unsafe_allow_html=True)
    if st.button("ë¡œê·¸ì•„ì›ƒ", key="logout_button_final_v4_mobile"):
        st.session_state["authenticated"] = False
        st.session_state["user"] = {}
        st.session_state["messages"] = []
        print("Logout successful. Chat messages cleared.")
        if 'cookies' in locals() and cookies and cookies.ready():
             try:
                 cookies["authenticated"] = "false"; cookies["user"] = ""; cookies["login_time"] = ""; cookies.save()
                 print("Cookies cleared on logout.")
             except Exception as e_logout_cookie:
                 print(f"ERROR: Failed to clear cookies on logout: {e_logout_cookie}")
        else:
              print("WARNING: CookieManager not ready during logout.")
        st.rerun()
    st.markdown('</div>', unsafe_allow_html=True)

# --- ë©”ì¸ ì•± ì œëª© (ë¡œê·¸ì¸ í›„) ---
# (ë©”ì¸ ì•± ì œëª© ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼)
st.markdown("""
<div class="main-app-title-container">
  <span class="main-app-title">ìœ ì•¤ìƒëª…ê³¼í•™ GMP/SOP ì—…ë¬´ ê°€ì´ë“œ ë´‡</span>
  <span class="main-app-subtitle">Made by DI.PART</span>
</div>
""", unsafe_allow_html=True)


# --- ë²¡í„° DB ë¡œë“œ (Azure Blob Storage ê¸°ë°˜) ---
# (ë²¡í„° DB ë¡œë“œ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼)
@st.cache_resource
def load_vector_db_from_blob_cached(_container_client):
    if not _container_client:
        print("ERROR: Blob Container client is None for load_vector_db_from_blob_cached.")
        return faiss.IndexFlatL2(1536), []
    idx, meta = faiss.IndexFlatL2(1536), []
    print(f"Attempting to load vector DB from Blob: '{INDEX_BLOB_NAME}', '{METADATA_BLOB_NAME}'")
    try:
        with tempfile.TemporaryDirectory() as tmpdir:
            local_index_path = os.path.join(tmpdir, os.path.basename(INDEX_BLOB_NAME))
            local_metadata_path = os.path.join(tmpdir, os.path.basename(METADATA_BLOB_NAME))

            index_blob_client = _container_client.get_blob_client(INDEX_BLOB_NAME)
            if index_blob_client.exists():
                print(f"Downloading '{INDEX_BLOB_NAME}'...")
                with open(local_index_path, "wb") as download_file:
                    download_stream = index_blob_client.download_blob(timeout=60)
                    download_file.write(download_stream.readall())
                if os.path.getsize(local_index_path) > 0:
                    idx = faiss.read_index(local_index_path)
                    print(f"'{INDEX_BLOB_NAME}' loaded successfully from Blob Storage.")
                else:
                    print(f"WARNING: '{INDEX_BLOB_NAME}' is empty in Blob. Using new index.")
            else:
                print(f"WARNING: '{INDEX_BLOB_NAME}' not found in Blob Storage. New index will be used/created.")

            metadata_blob_client = _container_client.get_blob_client(METADATA_BLOB_NAME)
            if metadata_blob_client.exists():
                print(f"Downloading '{METADATA_BLOB_NAME}'...")
                with open(local_metadata_path, "wb") as download_file:
                    download_stream = metadata_blob_client.download_blob(timeout=60)
                    download_file.write(download_stream.readall())
                if os.path.getsize(local_metadata_path) > 0 :
                    with open(local_metadata_path, "r", encoding="utf-8") as f: meta = json.load(f)
                else:
                    meta = []
                    print(f"WARNING: '{METADATA_BLOB_NAME}' is empty in Blob.")
            else:
                print(f"WARNING: '{METADATA_BLOB_NAME}' not found in Blob Storage. Starting with empty metadata.")
                meta = []
    except AzureError as ae:
        st.error(f"Azure Blobì—ì„œ ë²¡í„°DB ë¡œë“œ ì¤‘ Azure ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {ae}")
        print(f"AZURE ERROR loading vector DB from Blob: {ae}\n{traceback.format_exc()}")
    except Exception as e:
        st.error(f"Azure Blobì—ì„œ ë²¡í„°DB ë¡œë“œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        print(f"GENERAL ERROR loading vector DB from Blob: {e}\n{traceback.format_exc()}")
    return idx, meta

index, metadata = faiss.IndexFlatL2(1536), []
if container_client:
    index, metadata = load_vector_db_from_blob_cached(container_client)
else:
    st.error("Azure Blob Storage ì—°ê²° ì‹¤íŒ¨ë¡œ ë²¡í„° DBë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ í•™ìŠµ ë° ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("CRITICAL: Cannot load vector DB due to Blob client initialization failure (main section).")


# --- ê·œì¹™ íŒŒì¼ ë¡œë“œ ---
# (ê·œì¹™ íŒŒì¼ ë¡œë“œ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼ - ìµœì‹  í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
@st.cache_data
def load_prompt_rules_cached():
    # ì‚¬ìš©ìê°€ ì œê³µí•œ í”„ë¡¬í”„íŠ¸ ê·œì¹™ì„ íŒŒì¼ì—ì„œ ì½ê±°ë‚˜, ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
    # (ì´ ë¶€ë¶„ì€ ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€, ì‚¬ìš©ìê°€ ì§ì ‘ íŒŒì¼ì„ ìˆ˜ì •í•˜ëŠ” ê²ƒì„ ê°€ì •)
    default_rules = """1.ìš°ì„  ê¸°ì¤€
    1.1. ëª¨ë“  ë‹µë³€ì€ MFDS ê·œì •ì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ë©°, ê·¸ ë‹¤ìŒì€ ì‚¬ë‚´ SOPë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ìŠµë‹ˆë‹¤.
    1.2. ê·œì •/ë²•ë ¹ ìœ„ë°˜ ë˜ëŠ” íšŒìƒ‰ì§€ëŒ€ì˜ ê²½ìš°, ê´€ë ¨ ë¬¸ì„œëª…, ì¡°í•­ë²ˆí˜¸, ì¡°í•­ë‚´ìš©ê³¼ í•¨ê»˜ ëª…í™•íˆ ê²½ê³ í•´ì•¼ í•©ë‹ˆë‹¤.
    1.3. ëª…í™•í•œ ê·œì •ì´ ì—†ì„ ê²½ìš°, â€œë‚´ë¶€ QA ê²€í†  í•„ìš”â€ì„ì„ ê³ ì§€í•©ë‹ˆë‹¤.
2.ì‘ë‹µ ë°©ì‹
    2.1. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ë©°, ì „ë¬¸ì ì´ê³  ì¹œì ˆí•œ ì–´ì¡°ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
    2.2. ëª¨ë“  ë‹µë³€ì€ ë…¼ë¦¬ì  êµ¬ì¡°, ë†’ì€ ì •í™•ì„±, ì‹¤ìš©ì„±, ì˜ˆì‹œ ë° ì„¤ëª… í¬í•¨ ë“± ì „ë¬¸ê°€ ìˆ˜ì¤€ì„ ìœ ì§€í•©ë‹ˆë‹¤.
3.ê¸°ëŠ¥ ì œí•œ ë° íŒŒì¼ ì²˜ë¦¬
    3.1. ë‹¤ë£¨ëŠ” ì£¼ì œ: SOP, GMP ê°€ì´ë“œë¼ì¸(FDA, PIC/S, EU-GMP, cGMP, MFDS ë“±), DI ê·œì •, ì™¸êµ­ ê·œì • ë²ˆì—­ ë“± ì—…ë¬´ ê´€ë ¨ ë‚´ìš©.
    3.2. íŒŒì¼ ì²¨ë¶€ ì‹œ ì²˜ë¦¬:
        - ì‚¬ìš©ìê°€ íŒŒì¼ì„ ì²¨ë¶€í•˜ì—¬ ì§ˆë¬¸í•˜ëŠ” ê²½ìš°, í•´ë‹¹ íŒŒì¼ ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
        - ì§ˆë¬¸ ìœ í˜•ì´ ë²ˆì—­ì´ ì•„ë‹ˆë”ë¼ë„, íŒŒì¼ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½, ì„¤ëª…, ë¹„êµ ë“±ì˜ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
        - ì‚¬ìš©ìê°€ 'ì „ì²´ ë²ˆì—­'ì„ ìš”ì²­í•˜ëŠ” ê²½ìš°, ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ì „ì²´ ë²ˆì—­ ëŒ€ì‹  ì£¼ìš” ë‚´ìš© ìš”ì•½ ë° ë²ˆì—­ì„ ì œê³µí•˜ê±°ë‚˜, íŠ¹ì • ë¶€ë¶„ì„ ì§€ì •í•˜ì—¬ ì§ˆë¬¸í•˜ë„ë¡ ì•ˆë‚´í•©ë‹ˆë‹¤. (ì¶”ê°€ëœ ê·œì¹™)
        - ë§Œì•½ íŒŒì¼ ë‚´ìš©ì´ ê¸°ì¡´ MFDS ê·œì •ì´ë‚˜ ì‚¬ë‚´ SOPì™€ ìƒì¶©ë  ê°€ëŠ¥ì„±ì´ ìˆë‹¤ë©´, ê·¸ ì ì„ ëª…í™•íˆ ì–¸ê¸‰í•˜ê³  ì‚¬ìš©ìì—ê²Œ í™•ì¸ì„ ìš”ì²­í•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: "ì²¨ë¶€í•´ì£¼ì‹  ë¬¸ì„œì˜ ë‚´ìš©ì€ í˜„ì¬ SOPì™€ ì¼ë¶€ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    3.3. ì‚¬ìš©ìê°€ íŒŒì¼ì„ ì²¨ë¶€í•˜ê³  í•´ë‹¹ íŒŒì¼ì— ëŒ€í•´ ì§ˆë¬¸í•˜ëŠ” ê²½ìš°ë¥¼ ì œì™¸í•˜ê³ , ê·¸ ì™¸ ê°œì¸ì ì¸ ì§ˆë¬¸, ë‰´ìŠ¤, ì—¬ê°€ ë“± ì—…ë¬´ì™€ ì§ì ‘ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì€ â€œì—…ë¬´ ê´€ë ¨ ì§ˆë¬¸ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.â€ë¡œ ê°„ê²°íˆ ì‘ë‹µí•©ë‹ˆë‹¤. (ìˆ˜ì •ëœ ê·œì¹™)
4.ì±—ë´‡ ì†Œê°œ ì•ˆë‚´
    4.1. ì‚¬ìš©ìê°€ ì¸ì‚¬í•˜ê±°ë‚˜ ê¸°ëŠ¥ì„ ë¬¼ì„ ê²½ìš°, ë³¸ ì±—ë´‡ì˜ ì—­í• ê³¼ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì—…ë¬´ ë²”ìœ„ë¥¼ ê°„ë‹¨íˆ ì†Œê°œí•©ë‹ˆë‹¤.
5.í‘œí˜„ ë° í˜•ì‹ ê·œì¹™
    5.1. Markdown ìŠ¤íƒ€ì¼ ê°•ì¡°ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    5.2. ë²ˆí˜¸ í•­ëª©ì€ ë™ì¼í•œ ì„œì‹(ê¸€ê¼´ í¬ê¸°ì™€ êµµê¸°)ìœ¼ë¡œ í†µì¼í•©ë‹ˆë‹¤.
    5.3. ë‹µë³€ì€ í‘œ, ìš”ì•½, í•µì‹¬ ì •ë¦¬ ì¤‘ì‹¬ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ êµ¬ì„±í•©ë‹ˆë‹¤."""

    if os.path.exists(RULES_PATH_REPO):
        try:
            with open(RULES_PATH_REPO, "r", encoding="utf-8") as f: rules_content = f.read()
            print(f"Prompt rules loaded successfully from '{RULES_PATH_REPO}'.")
            return rules_content
        except Exception as e:
            st.warning(f"'{RULES_PATH_REPO}' íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}. ê¸°ë³¸ ê·œì¹™ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            print(f"WARNING: Error loading prompt rules from '{RULES_PATH_REPO}': {e}")
            return default_rules
    else:
        print(f"WARNING: Prompt rules file not found at '{RULES_PATH_REPO}'. Using default rules.")
        return default_rules

PROMPT_RULES_CONTENT = load_prompt_rules_cached()

# --- í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ---
# (extract_text_from_file, chunk_text_into_pieces, get_text_embedding, search_similar_chunks ë“± ì´ì „ê³¼ ë™ì¼)
def extract_text_from_file(uploaded_file_obj):
    ext = os.path.splitext(uploaded_file_obj.name)[1].lower(); text_content = ""
    try:
        uploaded_file_obj.seek(0); file_bytes = uploaded_file_obj.read()
        if ext == ".pdf":
            with fitz.open(stream=file_bytes, filetype="pdf") as doc: text_content = "\n".join(page.get_text() for page in doc)
        elif ext == ".docx":
            with io.BytesIO(file_bytes) as doc_io: doc = docx.Document(doc_io); text_content = "\n".join(para.text for para in doc.paragraphs)
        elif ext in (".xlsx", ".xlsm"):
            with io.BytesIO(file_bytes) as excel_io: df = pd.read_excel(excel_io, sheet_name=None) # ëª¨ë“  ì‹œíŠ¸ ì½ê¸°
            text_content = ""
            for sheet_name, sheet_df in df.items():
                 text_content += f"--- ì‹œíŠ¸: {sheet_name} ---\n{sheet_df.to_string(index=False)}\n\n" # ì‹œíŠ¸ ì´ë¦„ê³¼ í•¨ê»˜ ë‚´ìš© ì¶”ê°€
        elif ext == ".csv":
            with io.BytesIO(file_bytes) as csv_io:
                try: df = pd.read_csv(csv_io)
                except UnicodeDecodeError: df = pd.read_csv(csv_io, encoding='cp949')
                text_content = df.to_string(index=False)
        elif ext == ".pptx":
            with io.BytesIO(file_bytes) as ppt_io: prs = Presentation(ppt_io); text_content = "\n".join(shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, "text"))
        else: st.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}"); return ""
    except Exception as e:
        st.error(f"'{uploaded_file_obj.name}' íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"Error extracting text from '{uploaded_file_obj.name}': {e}\n{traceback.format_exc()}")
        return ""
    return text_content.strip()

def chunk_text_into_pieces(text_to_chunk, chunk_size=500): # ì²­í¬ í¬ê¸°ëŠ” ì„ë² ë”© ëª¨ë¸ì— ë§ê²Œ ì¡°ì ˆ ê°€ëŠ¥
    if not text_to_chunk or not text_to_chunk.strip(): return [];
    chunks_list, current_buffer = [], ""
    for line in text_to_chunk.split("\n"):
        stripped_line = line.strip()
        if not stripped_line and not current_buffer.strip(): continue
        if len(current_buffer) + len(stripped_line) + 1 < chunk_size:
            current_buffer += stripped_line + "\n"
        else:
            if current_buffer.strip(): chunks_list.append(current_buffer.strip())
            current_buffer = stripped_line + "\n"
    if current_buffer.strip(): chunks_list.append(current_buffer.strip())
    return [c for c in chunks_list if c]

def get_text_embedding(text_to_embed):
    if not openai_client or not EMBEDDING_MODEL:
        print("ERROR: OpenAI client or embedding model not ready for get_text_embedding (called).")
        return None
    if not text_to_embed or not text_to_embed.strip(): return None
    print(f"Requesting embedding for text: '{text_to_embed[:30]}...'")
    try:
        response = openai_client.embeddings.create(input=[text_to_embed], model=EMBEDDING_MODEL, timeout=AZURE_OPENAI_TIMEOUT / 2)
        print("Embedding received.")
        return response.data[0].embedding
    except APITimeoutError:
        st.error("í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘ ì‹œê°„ ì´ˆê³¼ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        print(f"TIMEOUT ERROR: Embedding request for '{text_to_embed[:50]}...' timed out.")
        return None
    except APIConnectionError as ace:
        st.error(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘ API ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {ace}. ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        print(f"API CONNECTION ERROR during embedding: {ace}")
        return None
    except RateLimitError as rle:
        st.error(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘ API ìš”ì²­ëŸ‰ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤: {rle}. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        print(f"RATE LIMIT ERROR during embedding: {rle}")
        return None
    except APIStatusError as ase:
        st.error(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘ API ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ {ase.status_code}): {ase.message}. ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.")
        print(f"API STATUS ERROR during embedding (Status {ase.status_code}): {ase.message}")
        return None
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        print(f"UNEXPECTED ERROR during embedding: {e}\n{traceback.format_exc()}")
        return None

def search_similar_chunks(query_text, k_results=5):
    if index is None or index.ntotal == 0 or not metadata:
        print("Search not possible: Index is empty or metadata is missing.")
        return []
    print(f"Searching for similar chunks for query: '{query_text[:30]}...'")
    query_vector = get_text_embedding(query_text)
    if query_vector is None:
        print("Failed to get query vector for similarity search.")
        return []
    try:
        actual_k = min(k_results, index.ntotal)
        if actual_k == 0 :
            print("No items in index to search.")
            return []

        distances, indices_found = index.search(np.array([query_vector]).astype("float32"), actual_k)
        valid_indices = [i for i in indices_found[0] if 0 <= i < len(metadata)]
        results = [metadata[i]["content"] for i in valid_indices]
        print(f"Similarity search found {len(results)} relevant chunks.")
        return results
    except Exception as e:
        st.error(f"ìœ ì‚¬ë„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"ERROR: Similarity search failed: {e}\n{traceback.format_exc()}")
        return []


# --- ë¬¸ì„œ ì¶”ê°€, ì›ë³¸ ì €ì¥, ì‚¬ìš©ëŸ‰ ë¡œê¹… í•¨ìˆ˜ ---
# (add_document_to_vector_db_and_blob, save_original_file_to_blob, log_openai_api_usage_to_blob í•¨ìˆ˜ëŠ” ì´ì „ê³¼ ë™ì¼)
def add_document_to_vector_db_and_blob(uploaded_file_obj, text_content, text_chunks, _container_client):
    global index, metadata
    if not text_chunks: st.warning(f"'{uploaded_file_obj.name}' íŒŒì¼ì—ì„œ ì²˜ë¦¬í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."); return False
    if not _container_client: st.error("Azure Blob í´ë¼ì´ì–¸íŠ¸ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ í•™ìŠµ ê²°ê³¼ë¥¼ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); return False

    vectors_to_add, new_metadata_entries_for_current_file = [], []
    for chunk_idx, chunk in enumerate(text_chunks):
        print(f"Processing chunk {chunk_idx+1}/{len(text_chunks)} for embedding from '{uploaded_file_obj.name}'...")
        embedding = get_text_embedding(chunk)
        if embedding is not None:
            vectors_to_add.append(embedding)
            new_metadata_entries_for_current_file.append({"file_name": uploaded_file_obj.name, "content": chunk})
        else:
            print(f"Warning: Failed to get embedding for a chunk in '{uploaded_file_obj.name}'. Skipping chunk.")

    if not vectors_to_add: st.warning(f"'{uploaded_file_obj.name}' íŒŒì¼ì—ì„œ ìœ íš¨í•œ ì„ë² ë”©ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."); return False

    try:
        if vectors_to_add: index.add(np.array(vectors_to_add).astype("float32"))
        metadata.extend(new_metadata_entries_for_current_file)
        print(f"Added {len(vectors_to_add)} new chunks to in-memory DB from '{uploaded_file_obj.name}'. Index total: {index.ntotal}")

        with tempfile.TemporaryDirectory() as tmpdir:
            temp_index_path = os.path.join(tmpdir, "temp.index")
            if index.ntotal > 0 :
                 faiss.write_index(index, temp_index_path)
                 if not save_binary_data_to_blob(temp_index_path, INDEX_BLOB_NAME, _container_client, "ë²¡í„° ì¸ë±ìŠ¤"):
                    st.error("ë²¡í„° ì¸ë±ìŠ¤ Blob ì €ì¥ ì‹¤íŒ¨"); return False
            else:
                print(f"Skipping saving empty index to Blob: {INDEX_BLOB_NAME}")

        if not save_data_to_blob(metadata, METADATA_BLOB_NAME, _container_client, "ë©”íƒ€ë°ì´í„°"):
            st.error("ë©”íƒ€ë°ì´í„° Blob ì €ì¥ ì‹¤íŒ¨"); return False

        user_info = st.session_state.get("user", {}); uploader_name = user_info.get("name", "N/A")
        new_log_entry = {"file": uploaded_file_obj.name, "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                         "chunks_added": len(vectors_to_add), "uploader": uploader_name}

        current_upload_logs = load_data_from_blob(UPLOAD_LOG_BLOB_NAME, _container_client, "ì—…ë¡œë“œ ë¡œê·¸", default_value=[])
        if not isinstance(current_upload_logs, list): current_upload_logs = []
        current_upload_logs.append(new_log_entry)
        if not save_data_to_blob(current_upload_logs, UPLOAD_LOG_BLOB_NAME, _container_client, "ì—…ë¡œë“œ ë¡œê·¸"):
            st.warning("ì—…ë¡œë“œ ë¡œê·¸ë¥¼ Blobì— ì €ì¥í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        st.error(f"ë¬¸ì„œ í•™ìŠµ ë˜ëŠ” Azure Blob ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"ERROR: Failed to add document or upload to Blob: {e}\n{traceback.format_exc()}")
        return False

def save_original_file_to_blob(uploaded_file_obj, _container_client):
    if not _container_client: st.error("Azure Blob í´ë¼ì´ì–¸íŠ¸ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ì›ë³¸ íŒŒì¼ì„ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); return None
    try:
        uploaded_file_obj.seek(0)
        original_blob_name = f"uploaded_originals/{datetime.now().strftime('%Y%m%d%H%M%S')}_{uploaded_file_obj.name}"
        blob_client_for_original = _container_client.get_blob_client(blob=original_blob_name)
        blob_client_for_original.upload_blob(uploaded_file_obj.getvalue(), overwrite=False, timeout=120)
        print(f"Original file '{uploaded_file_obj.name}' saved to Blob as '{original_blob_name}'.")
        return original_blob_name
    except AzureError as ae:
        st.error(f"'{uploaded_file_obj.name}' ì›ë³¸ íŒŒì¼ Blob ì—…ë¡œë“œ ì¤‘ Azure ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {ae}")
        print(f"AZURE ERROR saving original file to Blob: {ae}\n{traceback.format_exc()}")
        return None
    except Exception as e:
        st.error(f"'{uploaded_file_obj.name}' ì›ë³¸ íŒŒì¼ Blob ì—…ë¡œë“œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        print(f"GENERAL ERROR saving original file to Blob: {e}\n{traceback.format_exc()}")
        return None

def log_openai_api_usage_to_blob(user_id_str, model_name_str, usage_stats_obj, _container_client):
    if not _container_client:
        print("ERROR: Blob Container client is None for API usage log. Skipping log.")
        return

    prompt_tokens = getattr(usage_stats_obj, 'prompt_tokens', 0)
    completion_tokens = getattr(usage_stats_obj, 'completion_tokens', 0)
    total_tokens = getattr(usage_stats_obj, 'total_tokens', 0)

    new_log_entry = {
        "user_id": user_id_str, "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "model_used": model_name_str, "prompt_tokens": prompt_tokens,
        "completion_tokens": completion_tokens, "total_tokens": total_tokens
    }

    current_usage_logs = load_data_from_blob(USAGE_LOG_BLOB_NAME, _container_client, "API ì‚¬ìš©ëŸ‰ ë¡œê·¸", default_value=[])
    if not isinstance(current_usage_logs, list): current_usage_logs = []
    current_usage_logs.append(new_log_entry)

    if not save_data_to_blob(current_usage_logs, USAGE_LOG_BLOB_NAME, _container_client, "API ì‚¬ìš©ëŸ‰ ë¡œê·¸"):
        print(f"WARNING: Failed to save API usage log to Blob for user '{user_id_str}'.")


# --- ë©”ì¸ UI êµ¬ì„± ---
tab_labels_list = ["ğŸ’¬ ì—…ë¬´ ì§ˆë¬¸"]
if current_user_info.get("role") == "admin":
    tab_labels_list.append("âš™ï¸ ê´€ë¦¬ì ì„¤ì •")

main_tabs_list = st.tabs(tab_labels_list)
chat_interface_tab = main_tabs_list[0]
admin_settings_tab = main_tabs_list[1] if len(main_tabs_list) > 1 else None

with chat_interface_tab:
    st.header("ì—…ë¬´ ì§ˆë¬¸")
    st.markdown("ğŸ’¡ ì˜ˆì‹œ: SOP ë°±ì—… ì£¼ê¸°, PIC/S Annex 11 ì°¨ì´ ë“±")

    if "messages" not in st.session_state:
        st.session_state["messages"] = []
        print("Chat messages list re-initialized in chat_tab (should not happen if init is correct).")

    for msg_item in st.session_state["messages"]:
        role, content, time_str = msg_item.get("role"), msg_item.get("content", ""), msg_item.get("time", "")
        align_class = "user-align" if role == "user" else "assistant-align"
        bubble_class = "user-bubble" if role == "user" else "assistant-bubble"
        st.markdown(f"""<div class="chat-bubble-container {align_class}"><div class="bubble {bubble_class}">{content}</div><div class="timestamp">{time_str}</div></div>""", unsafe_allow_html=True)

    st.markdown("<div style='height:16px'></div>", unsafe_allow_html=True)
    if st.button("ğŸ“‚ íŒŒì¼ ì²¨ë¶€/ìˆ¨ê¸°ê¸°", key="toggle_chat_uploader_final_v4_button"):
        st.session_state.show_uploader = not st.session_state.get("show_uploader", False)

    chat_file_uploader_key = "chat_file_uploader_final_v4_widget"
    uploaded_chat_file_runtime = None
    if st.session_state.get("show_uploader", False):
        uploaded_chat_file_runtime = st.file_uploader("ì§ˆë¬¸ê³¼ í•¨ê»˜ ì°¸ê³ í•  íŒŒì¼ ì²¨ë¶€ (ì„ íƒ ì‚¬í•­)",
                                     type=["pdf","docx","xlsx","xlsm","csv","pptx"],
                                     key=chat_file_uploader_key)
        if uploaded_chat_file_runtime: st.caption(f"ì²¨ë¶€ë¨: {uploaded_chat_file_runtime.name}")

    with st.form("chat_input_form_final_v4", clear_on_submit=True):
        query_input_col, send_button_col = st.columns([4,1])
        with query_input_col:
            user_query_input = st.text_input("ì§ˆë¬¸ ì…ë ¥:", placeholder="ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...",
                                             key="user_query_text_input_final_v4", label_visibility="collapsed")
        with send_button_col:
            send_query_button = st.form_submit_button("ì „ì†¡")

    if send_query_button and user_query_input.strip():
        if not openai_client:
            st.error("OpenAI ì„œë¹„ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        elif not tokenizer: # Tiktoken ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬
             st.error("Tiktoken ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            timestamp_now_str = datetime.now().strftime("%Y-%m-%d %H:%M")
            st.session_state["messages"].append({"role":"user", "content":user_query_input, "time":timestamp_now_str})

            user_id_for_log = current_user_info.get("name", "anonymous_chat_user_runtime")

            print(f"User '{user_id_for_log}' submitted query: '{user_query_input[:50]}...'")
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
                assistant_response_content = "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                try:
                    print("Step 1: Preparing context and calculating tokens...")
                    context_chunks_for_prompt = []
                    temp_file_text = ""

                    # --- ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë° ì§ˆë¬¸ í† í° ê³„ì‚° ---
                    prompt_structure = f"{PROMPT_RULES_CONTENT}\n\nìœ„ì˜ ê·œì¹™ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë° ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œì˜ ë‚´ìš©ì…ë‹ˆë‹¤:\n<ë¬¸ì„œ ì‹œì‘>\n{{context}}\n<ë¬¸ì„œ ë>"
                    base_prompt_text = prompt_structure.replace('{context}', '')
                    try:
                        base_tokens = len(tokenizer.encode(base_prompt_text))
                        query_tokens = len(tokenizer.encode(user_query_input))
                    except Exception as e:
                        st.error(f"ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë˜ëŠ” ì§ˆë¬¸ í† í°í™” ì¤‘ ì˜¤ë¥˜: {e}")
                        raise # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë‹¨

                    print(f"DEBUG: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° í† í°: {base_tokens}")
                    print(f"DEBUG: ì‚¬ìš©ì ì§ˆë¬¸ í† í°: {query_tokens}")

                    max_context_tokens = TARGET_INPUT_TOKENS_FOR_PROMPT - base_tokens - query_tokens
                    print(f"DEBUG: ëª©í‘œ í”„ë¡¬í”„íŠ¸ í† í°: {TARGET_INPUT_TOKENS_FOR_PROMPT}")
                    print(f"DEBUG: ì»¨í…ìŠ¤íŠ¸ì— í• ë‹¹ ê°€ëŠ¥í•œ ìµœëŒ€ í† í°: {max_context_tokens}")

                    if max_context_tokens <= 0:
                         st.warning("í”„ë¡¬í”„íŠ¸ ê·œì¹™ê³¼ ì§ˆë¬¸ë§Œìœ¼ë¡œë„ ì…ë ¥ í† í° ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì²¨ë¶€íŒŒì¼ ë‚´ìš©ì„ í¬í•¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                         print("WARNING: No tokens left for context after accounting for rules and query.")
                         context_string_for_llm = "ì°¸ê³ í•  ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (í† í° ì œí•œ)."
                    else:
                        # --- ì»¨í…ìŠ¤íŠ¸ ìƒì„± (íŒŒì¼ ë‚´ìš© ìš°ì„ ) ---
                        if uploaded_chat_file_runtime:
                            print(f"Processing uploaded file: {uploaded_chat_file_runtime.name}")
                            temp_file_text = extract_text_from_file(uploaded_chat_file_runtime)
                            print(f"DEBUG: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ì• 100ì): {temp_file_text[:100] if temp_file_text else 'ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ íŒŒì¼'}")

                            if temp_file_text:
                                context_chunks_for_prompt.append(temp_file_text)
                                print(f"DEBUG: íŒŒì¼ ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ ì²­í¬ í›„ë³´ì— ì¶”ê°€.")
                            else: st.info(f"'{uploaded_chat_file_runtime.name}' íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.")

                        # --- ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ë²¡í„° ê²€ìƒ‰ ë³´ì¡° - í˜„ì¬ëŠ” ë¹„í™œì„±í™”) ---
                        # (í•„ìš” ì‹œ ì´ ë¶€ë¶„ì— ë²¡í„° ê²€ìƒ‰ ë¡œì§ ì¶”ê°€)

                        # --- ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ ìƒì„± ë° í† í° ê¸°ë°˜ ìë¥´ê¸° ---
                        final_unique_context = list(dict.fromkeys(c for c in context_chunks_for_prompt if c and c.strip()))
                        if not final_unique_context:
                            st.info("ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì°¸ê³  ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë‹µë³€ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                            context_string_for_llm = "í˜„ì¬ ì°¸ê³ í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
                        else:
                            full_context_string = "\n\n---\n\n".join(final_unique_context)
                            try:
                                full_context_tokens = tokenizer.encode(full_context_string)
                            except Exception as e:
                                st.error(f"ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ í† í°í™” ì¤‘ ì˜¤ë¥˜: {e}")
                                raise

                            print(f"DEBUG: ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ í† í° ìˆ˜: {len(full_context_tokens)}")

                            if len(full_context_tokens) > max_context_tokens:
                                truncated_tokens = full_context_tokens[:max_context_tokens]
                                try:
                                    context_string_for_llm = tokenizer.decode(truncated_tokens)
                                except Exception as e:
                                     st.error(f"ì˜ë¦° í† í° ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜: {e}")
                                     # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬ ë˜ëŠ” ë‹¤ë¥¸ ëŒ€ì²´ ë¡œì§ í•„ìš”
                                     context_string_for_llm = "[ì˜¤ë¥˜: ì»¨í…ìŠ¤íŠ¸ ë””ì½”ë”© ì‹¤íŒ¨]"
                                print(f"WARNING: ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜ê°€ ë„ˆë¬´ ë§ì•„ {max_context_tokens} í† í°ìœ¼ë¡œ ì˜ëìŠµë‹ˆë‹¤.")
                                print(f"DEBUG: ì˜ë¦° ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ì• 100ì): {context_string_for_llm[:100]}")
                            else:
                                context_string_for_llm = full_context_string
                                print(f"DEBUG: ì „ì²´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

                    # --- ìµœì¢… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ---
                    system_prompt_content = prompt_structure.replace('{context}', context_string_for_llm)
                    try:
                        final_system_tokens = len(tokenizer.encode(system_prompt_content))
                        final_prompt_tokens = final_system_tokens + query_tokens
                    except Exception as e:
                         st.error(f"ìµœì¢… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í† í°í™” ì¤‘ ì˜¤ë¥˜: {e}")
                         raise

                    print(f"DEBUG: ìµœì¢… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í† í°: {final_system_tokens}")
                    print(f"DEBUG: ìµœì¢… API ì…ë ¥ í† í° (ì‹œìŠ¤í…œ+ì§ˆë¬¸): {final_prompt_tokens}")
                    if final_prompt_tokens > MODEL_MAX_INPUT_TOKENS:
                         print(f"CRITICAL WARNING: ìµœì¢… ì…ë ¥ í† í°({final_prompt_tokens})ì´ ëª¨ë¸ ìµœëŒ€ì¹˜({MODEL_MAX_INPUT_TOKENS})ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤! API ì˜¤ë¥˜ ê°€ëŠ¥ì„± ë†’ìŒ.")

                    chat_messages = [{"role":"system", "content": system_prompt_content}, {"role":"user", "content": user_query_input}]

                    print("Step 2: Sending request to Azure OpenAI...")
                    chat_completion_response = openai_client.chat.completions.create(
                        model=st.secrets["AZURE_OPENAI_DEPLOYMENT"],
                        messages=chat_messages,
                        max_tokens=MODEL_MAX_OUTPUT_TOKENS,
                        temperature=0.1,
                        timeout=AZURE_OPENAI_TIMEOUT
                    )
                    assistant_response_content = chat_completion_response.choices[0].message.content.strip()
                    print("Azure OpenAI response received.")

                    if chat_completion_response.usage and container_client:
                        print("Logging OpenAI API usage...")
                        log_openai_api_usage_to_blob(user_id_for_log, st.secrets["AZURE_OPENAI_DEPLOYMENT"], chat_completion_response.usage, container_client)

                # ... (ì´í•˜ ì—ëŸ¬ ì²˜ë¦¬ ë° ë©”ì‹œì§€ ì¶”ê°€ ë¡œì§ì€ ë™ì¼) ...
                except APITimeoutError:
                    assistant_response_content = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” ê°„ë‹¨í•˜ê²Œ í•´ì£¼ì‹œê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    st.error(assistant_response_content)
                    print(f"TIMEOUT ERROR: Chat completion request timed out for user '{user_id_for_log}'.")
                except APIConnectionError as ace:
                    assistant_response_content = f"API ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {ace}. ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    st.error(assistant_response_content)
                    print(f"API CONNECTION ERROR during chat completion: {ace}")
                except RateLimitError as rle:
                    assistant_response_content = f"API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {rle}. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    st.error(assistant_response_content)
                    print(f"RATE LIMIT ERROR during chat completion: {rle}")
                except APIStatusError as ase:
                    assistant_response_content = f"APIì—ì„œ ì˜¤ë¥˜ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤ (ìƒíƒœ ì½”ë“œ {ase.status_code}): {ase.message}. ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
                    st.error(assistant_response_content)
                    print(f"API STATUS ERROR during chat completion (Status {ase.status_code}): {ase.message}")
                except Exception as gen_err:
                    assistant_response_content = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {gen_err}. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
                    st.error(assistant_response_content)
                    print(f"UNEXPECTED ERROR during response generation: {gen_err}\n{traceback.format_exc()}")

                st.session_state["messages"].append({"role":"assistant", "content":assistant_response_content, "time":timestamp_now_str})
                print("Response processing complete.")
            st.rerun()


if admin_settings_tab:
    with admin_settings_tab:
        # (ê´€ë¦¬ì íƒ­ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼)
        st.header("âš™ï¸ ê´€ë¦¬ì ì„¤ì •")
        st.subheader("ğŸ‘¥ ê°€ì… ìŠ¹ì¸ ëŒ€ê¸°ì")
        if not USERS or not isinstance(USERS, dict):
            st.warning("ì‚¬ìš©ì ì •ë³´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print(f"WARNING: USERS data is problematic or empty in admin tab. Type: {type(USERS)}")
        else:
            pending_approval_users = {uid:udata for uid,udata in USERS.items() if isinstance(udata, dict) and not udata.get("approved")}
            if pending_approval_users:
                for pending_uid, pending_user_data in pending_approval_users.items():
                    with st.expander(f"{pending_user_data.get('name','N/A')} ({pending_uid}) - {pending_user_data.get('department','N/A')}"):
                        approve_col, reject_col = st.columns(2)
                        if approve_col.button("ìŠ¹ì¸", key=f"admin_approve_user_final_v4_{pending_uid}"):
                            USERS[pending_uid]["approved"] = True
                            if save_data_to_blob(USERS, USERS_BLOB_NAME, container_client, "ì‚¬ìš©ì ì •ë³´"):
                                st.success(f"'{pending_uid}' ì‚¬ìš©ìë¥¼ ìŠ¹ì¸í•˜ê³  Blobì— ì €ì¥í–ˆìŠµë‹ˆë‹¤."); st.rerun()
                            else: st.error("ì‚¬ìš©ì ìŠ¹ì¸ ì •ë³´ Blob ì €ì¥ ì‹¤íŒ¨.")
                        if reject_col.button("ê±°ì ˆ", key=f"admin_reject_user_final_v4_{pending_uid}"):
                            USERS.pop(pending_uid, None)
                            if save_data_to_blob(USERS, USERS_BLOB_NAME, container_client, "ì‚¬ìš©ì ì •ë³´"):
                                st.info(f"'{pending_uid}' ì‚¬ìš©ìì˜ ê°€ì… ì‹ ì²­ì„ ê±°ì ˆí•˜ê³  Blobì— ì €ì¥í–ˆìŠµë‹ˆë‹¤."); st.rerun()
                            else: st.error("ì‚¬ìš©ì ê±°ì ˆ ì •ë³´ Blob ì €ì¥ ì‹¤íŒ¨.")
            else: st.info("ìŠ¹ì¸ ëŒ€ê¸° ì¤‘ì¸ ì‚¬ìš©ìê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.markdown("---")

        st.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ë° í•™ìŠµ (Azure Blob Storage)")
        admin_file_uploader_key = "admin_file_uploader_final_v4_widget"
        admin_uploaded_file = st.file_uploader("í•™ìŠµí•  íŒŒì¼ ì—…ë¡œë“œ", type=["pdf","docx","xlsx","xlsm","csv","pptx"], key=admin_file_uploader_key)

        if admin_uploaded_file and container_client:
            with st.spinner(f"'{admin_uploaded_file.name}' íŒŒì¼ ì²˜ë¦¬ ë° í•™ìŠµ ì¤‘..."):
                extracted_content = extract_text_from_file(admin_uploaded_file)
                if extracted_content:
                    content_chunks = chunk_text_into_pieces(extracted_content) # ì²­í‚¹ì€ ì„ë² ë”©ì„ ìœ„í•´ ìœ ì§€
                    if content_chunks:
                        original_file_blob_path = save_original_file_to_blob(admin_uploaded_file, container_client)
                        if original_file_blob_path: st.caption(f"ì›ë³¸ íŒŒì¼ì´ Blobì— '{original_file_blob_path}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        else: st.warning("ì›ë³¸ íŒŒì¼ì„ Blobì— ì €ì¥í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

                        # ë²¡í„° DB ì¶”ê°€ ë¡œì§ì€ ì„ë² ë”© ëª¨ë¸ ê¸°ë°˜ì´ë¯€ë¡œ content_chunks ì‚¬ìš©
                        if add_document_to_vector_db_and_blob(admin_uploaded_file, extracted_content, content_chunks, container_client):
                            st.success(f"'{admin_uploaded_file.name}' íŒŒì¼ í•™ìŠµ ë° Azure Blob Storageì— ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
                        else: st.error(f"'{admin_uploaded_file.name}' í•™ìŠµ ë˜ëŠ” Blob ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                    else: st.warning(f"'{admin_uploaded_file.name}' íŒŒì¼ì—ì„œ ìœ ì˜ë¯¸í•œ ì²­í¬ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                else: st.warning(f"'{admin_uploaded_file.name}' íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.")
            st.rerun() # ì²˜ë¦¬ í›„ ìë™ ìƒˆë¡œê³ ì¹¨
        elif admin_uploaded_file and not container_client:
            st.error("Azure Blob í´ë¼ì´ì–¸íŠ¸ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  í•™ìŠµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.markdown("---")

        st.subheader("ğŸ“Š API ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (Blob ë¡œê·¸ ê¸°ë°˜)")
        if container_client:
            usage_data_from_blob = load_data_from_blob(USAGE_LOG_BLOB_NAME, container_client, "API ì‚¬ìš©ëŸ‰ ë¡œê·¸", default_value=[])
            if usage_data_from_blob and isinstance(usage_data_from_blob, list) and len(usage_data_from_blob) > 0 :
                df_usage_stats=pd.DataFrame(usage_data_from_blob)
                # ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
                for col in ["total_tokens", "prompt_tokens", "completion_tokens"]:
                     if col not in df_usage_stats.columns:
                         df_usage_stats[col] = 0

                # í† í° ì»¬ëŸ¼ ìˆ«ìí˜• ë³€í™˜ ë° NaN ì²˜ë¦¬
                token_cols = ["total_tokens", "prompt_tokens", "completion_tokens"]
                for col in token_cols:
                    df_usage_stats[col] = pd.to_numeric(df_usage_stats[col], errors='coerce').fillna(0)

                total_tokens_used = df_usage_stats["total_tokens"].sum()
                st.metric("ì´ API í˜¸ì¶œ ìˆ˜", len(df_usage_stats))
                st.metric("ì´ ì‚¬ìš© í† í° ìˆ˜", f"{int(total_tokens_used):,}")

                token_cost_per_unit = 0.0
                try: token_cost_per_unit=float(st.secrets.get("TOKEN_COST","0"))
                except (ValueError, TypeError): pass
                st.metric("ì˜ˆìƒ ë¹„ìš© (USD)", f"${total_tokens_used * token_cost_per_unit:.4f}") # Use float for calc

                if "timestamp" in df_usage_stats.columns:
                    # timestamp ì»¬ëŸ¼ì´ datetime ê°ì²´ê°€ ì•„ë‹ ê²½ìš° ë³€í™˜ ì‹œë„
                    try:
                         df_usage_stats['timestamp'] = pd.to_datetime(df_usage_stats['timestamp'])
                         st.dataframe(df_usage_stats.sort_values(by="timestamp",ascending=False), use_container_width=True)
                    except Exception as e:
                         print(f"Warning: Could not sort usage log by timestamp due to conversion error: {e}")
                         st.dataframe(df_usage_stats, use_container_width=True) # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ í‘œì‹œ
                else:
                    st.dataframe(df_usage_stats, use_container_width=True)
            else: st.info("ê¸°ë¡ëœ API ì‚¬ìš©ëŸ‰ ë°ì´í„°ê°€ Blobì— ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        else: st.warning("Azure Blob í´ë¼ì´ì–¸íŠ¸ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ API ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.markdown("---")

        st.subheader("ğŸ“‚ Azure Blob Storage íŒŒì¼ ëª©ë¡ (ìµœê·¼ 100ê°œ)")
        if container_client:
            try:
                blob_list_display = []
                count = 0
                max_blobs_to_show = 100
                # Blob ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ ìˆ˜ì • ì‹œê°„ìœ¼ë¡œ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
                blobs_sorted = sorted(container_client.list_blobs(), key=lambda b: b.last_modified, reverse=True)

                for blob_item in blobs_sorted:
                    if count >= max_blobs_to_show:
                        break
                    blob_list_display.append({
                        "íŒŒì¼ëª…": blob_item.name,
                        "í¬ê¸° (bytes)": blob_item.size,
                        "ìˆ˜ì •ì¼": blob_item.last_modified.strftime('%Y-%m-%d %H:%M:%S') if blob_item.last_modified else 'N/A'
                    })
                    count += 1

                if blob_list_display:
                    df_blobs_display = pd.DataFrame(blob_list_display)
                    st.dataframe(df_blobs_display, use_container_width=True) # ì´ë¯¸ ì •ë ¬ë¨
                else: st.info("Azure Blob Storageì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"Azure Blob íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
                print(f"ERROR listing blobs: {e}\n{traceback.format_exc()}")
        else:
            st.warning("Azure Blob í´ë¼ì´ì–¸íŠ¸ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ íŒŒì¼ ëª©ë¡ì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
